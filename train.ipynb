{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f67118",
   "metadata": {},
   "source": [
    "## Giới thiệu và Nhập Thư viện\n",
    "\n",
    "- **numpy**: Thư viện để làm việc với mảng và tính toán số học.\n",
    "- **random**: Dùng để tạo số ngẫu nhiên (không sử dụng trực tiếp trong code này).\n",
    "- **json**: Dùng để đọc file JSON chứa dữ liệu intents.\n",
    "- **torch**: Thư viện PyTorch để xây dựng và huấn luyện mô hình học sâu.\n",
    "- **torch.nn**: Chứa các thành phần mạng nơ-ron như lớp, hàm mất mát.\n",
    "- **torch.utils.data**: Chứa các công cụ để xử lý dữ liệu như Dataset, DataLoader.\n",
    "- **nltk_utils**: Module tùy chỉnh chứa các hàm xử lý văn bản như tokenize, stem, bag_of_words.\n",
    "- **model**: Module chứa lớp NeuralNet định nghĩa kiến trúc mạng nơ-ron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb279cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nltk_utils import bag_of_words, tokenize, stem\n",
    "from model import NeuralNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a77b8",
   "metadata": {},
   "source": [
    "## Tải và Xử lý Sơ bộ Dữ liệu Ý định (Intents)\n",
    "\n",
    "- Đọc file `intents.json` để lấy danh sách các ý định.\n",
    "- Tạo danh sách:\n",
    "  - `all_words`: Lưu tất cả các từ từ patterns.\n",
    "  - `tags`: Lưu các nhãn ý định.\n",
    "  - `xy`: Lưu cặp (câu tokenized, tag).\n",
    "  - `all_sentences`: Lưu tất cả các câu tokenized để tính IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22ccfce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json', encoding='utf-8') as f:\n",
    "    intents = json.load(f)\n",
    "\n",
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "all_sentences = []\n",
    "for intent in intents['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    for pattern in intent['patterns']:\n",
    "        w = tokenize(pattern)\n",
    "        all_words.extend(w)\n",
    "        xy.append((w, tag))\n",
    "        all_sentences.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab443b",
   "metadata": {},
   "source": [
    "## Tiền xử lý Dữ liệu Văn bản\n",
    "\n",
    "- Loại bỏ các ký tự không cần thiết (`ignore_words`).\n",
    "- Stem và chuyển các từ thành chữ thường.\n",
    "- Loại bỏ từ trùng lặp và sắp xếp `all_words` và `tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed815cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 patterns\n",
      "12 tags: ['delivery', 'feedback', 'funny', 'goodbye', 'greeting', 'hours', 'items', 'location', 'payments', 'reservation', 'special_request', 'thanks']\n",
      "196 unique stemmed words: [',', '5', 'ai', 'bao', 'biết', 'biệt', 'buổi', 'bàn', 'bán', 'bâi', 'bạn', 'bản', 'bật', 'bằng', 'bị', 'cay', 'chay', 'chi', 'cho', 'chuyện', 'chuẩn', 'chào', 'chúc', 'chấp', 'chỉ', 'cuối', 'các', 'cách', 'câu', 'có', 'cười', 'cảm', 'cần', 'cầu', 'cọc', 'của', 'cửa', 'dị', 'dịch', 'dụng', 'gian', 'giao', 'giá', 'giúp', 'giới', 'giờ', 'gì', 'góp', 'gần', 'gặp', 'gửi', 'hay', 'hoặc', 'hài', 'hàng', 'hành', 'hãi', 'hôm', 'hơn', 'hước', 'hẹn', 'hồi', 'hữu', 'khen', 'khi', 'khác', 'khách', 'không', 'khỏe', 'kiến', 'kể', 'liên', 'là', 'làm', 'lâu', 'lòng', 'lưu', 'lại', 'lần', 'lễ', 'mastercard', 'menu', 'momo', 'muối', 'muốn', 'món', 'mấi', 'mất', 'mặt', 'một', 'mới', 'mở', 'nay', 'nghe', 'ngài', 'người', 'ngợi', 'nhanh', 'nhiều', 'nhà', 'nhánh', 'nhé', 'nhóm', 'nhất', 'nhận', 'nhật', 'những', 'nài', 'nào', 'nói', 'nơi', 'nằm', 'nếu', 'nổi', 'phí', 'phản', 'phố', 'phộng', 'qua', 'quan', 'quay', 'ramen', 'rất', 'sao', 'sau', 'sushi', 'sáng', 'sẽ', 'sớm', 'sự', 'thanh', 'thiệu', 'thành', 'thêm', 'thì', 'thích', 'thông', 'thật', 'thẻ', 'thế', 'thể', 'thời', 'tin', 'tiền', 'tiện', 'toán', 'trung', 'trước', 'trả', 'trống', 'tuyệt', 'tuần', 'tâm', 'tín', 'tôi', 'tạm', 'tận', 'tối', 'tốn', 'tử', 'visa', 'vui', 'và', 'vào', 'vì', 'ví', 'vậi', 'về', 'với', 'vụ', 'xin', 'yêu', 'zalopay', 'ích', 'ít', 'ý', 'ăn', 'đi', 'điện', 'đánh', 'đâi', 'đâu', 'đó', 'đùa', 'được', 'đậu', 'đặc', 'đặt', 'đến', 'để', 'địa', 'đồ', 'đỡ', 'ơn', 'ở', 'ứng']\n"
     ]
    }
   ],
   "source": [
    "ignore_words = ['?', '.', '!']\n",
    "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "\n",
    "print(len(xy), \"patterns\")\n",
    "print(len(tags), \"tags:\", tags)\n",
    "print(len(all_words), \"unique stemmed words:\", all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2085f8d",
   "metadata": {},
   "source": [
    "## Tạo Dữ liệu Huấn luyện\n",
    "\n",
    "- Tạo `X_train` (vector TF-IDF) và `y_train` (nhãn).\n",
    "- Sử dụng hàm `bag_of_words` với `all_sentences` để tính TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad76f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for (pattern_sentence, tag) in xy:\n",
    "    bag = bag_of_words(pattern_sentence, all_words, all_sentences)\n",
    "    X_train.append(bag)\n",
    "    label = tags.index(tag)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a055cab",
   "metadata": {},
   "source": [
    "## Định nghĩa Siêu tham số và Kích thước Mô hình\n",
    "\n",
    "- Định nghĩa các siêu tham số: số epoch, kích thước batch, tốc độ học, v.v.\n",
    "- Xác định kích thước đầu vào và đầu ra dựa trên dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4158e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size: 196\n",
      "output_size: 12\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 800\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "input_size = len(X_train[0])\n",
    "hidden_size = 8\n",
    "output_size = len(tags)\n",
    "print(\"input_size:\", input_size)\n",
    "print(\"output_size:\", output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d268c4c",
   "metadata": {},
   "source": [
    "## Tạo Lớp ChatDataset\n",
    "\n",
    "- Tạo lớp `ChatDataset` để quản lý dữ liệu huấn luyện.\n",
    "- Hỗ trợ lập chỉ mục và trả về kích thước dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b2ae07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b35592",
   "metadata": {},
   "source": [
    "## Tạo DataLoader\n",
    "\n",
    "- Tạo `DataLoader` để chia dữ liệu thành các batch và xáo trộn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdb39274",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37a5d0",
   "metadata": {},
   "source": [
    "## Khởi tạo Mô hình, Hàm Mất mát và Trình Tối ưu hóa\n",
    "\n",
    "- Khởi tạo mô hình `NeuralNet` và chuyển sang thiết bị (`cuda` hoặc `cpu`).\n",
    "- Sử dụng `CrossEntropyLoss` và tối ưu hóa `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1bd9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c3085",
   "metadata": {},
   "source": [
    "## Huấn luyện Mô hình\n",
    "\n",
    "- Lặp qua các epoch, thực hiện forward pass, backward pass, và cập nhật tham số.\n",
    "- In giá trị hàm mất mát sau mỗi 100 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c80280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/800], Loss: 0.5040\n",
      "Epoch [200/800], Loss: 0.2754\n",
      "Epoch [300/800], Loss: 0.0208\n",
      "Epoch [400/800], Loss: 0.0066\n",
      "Epoch [500/800], Loss: 0.0041\n",
      "Epoch [600/800], Loss: 0.0033\n",
      "Epoch [700/800], Loss: 0.0008\n",
      "Epoch [800/800], Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for (words, labels) in train_loader:\n",
    "        words = words.to(device)\n",
    "        labels = labels.to(dtype=torch.long).to(device)\n",
    "        \n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc3223",
   "metadata": {},
   "source": [
    "## In Mất mát Cuối cùng và Lưu Mô hình\n",
    "\n",
    "- In giá trị hàm mất mát cuối cùng.\n",
    "- Lưu trạng thái mô hình và thông tin vào file `data.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a6f9404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss: 0.0006\n",
      "training complete. file saved to data.pth\n"
     ]
    }
   ],
   "source": [
    "print(f'final loss: {loss.item():.4f}')\n",
    "\n",
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": all_words,\n",
    "\"tags\": tags\n",
    "}\n",
    "\n",
    "FILE = \"data.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a302a-a6c8-4711-95f2-e3aee16887c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6d623-4b88-4b1e-b457-56b32bbd0621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202f594-1ab8-450a-8561-114f5e7b0dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
